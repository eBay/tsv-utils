# Tips and Tricks

Contents:

* [A faster way to unique a file](#A faster way to unique a file)
* [Reading data in R](#Reading data in R)
* [Using grep as a pre-filter](#Using grep as a pre-filter)

## A faster way to unique a file

The commands `sort | uniq` and `sort -u` are common ways to remove duplicates from a unsorted file. However, `tsv-uniq` is faster, generally by quite a bit. As a bonus, it preserves the original input order, including the header line. The following commands are equivalent, apart from sort order:
```
$ sort data.txt | uniq > data_unique.txt
$ sort -u data.txt > data_unique.txt
$ tsv-uniq data.txt > data_unique.txt
```

Run-times for the above commands are show below. Two different files were used, one 12 MB, 500,000 lines, the other 127 MB, 5 million lines. The files contained 339,185 and 3,394,172 unique lines respectively. Timing was done on a Macbook Pro with 16 GB of memory and flash storage. The `sort` and `uniq` programs are from GNU coreutils version 8.26. Run-times using `tsv-uniq` are nearly 10 times faster in examples.

| Command                 | File size         | Time (seconds) |
| ----------------------- | ----------------- | -------------: |
| `sort data.txt \| uniq` | 12 MB; 500K lines |           2.19 |
| `sort -u data.txt`      | 12 MB; 500K lines |           2.37 |
| `tsv-uniq data.txt`     | 12 MB; 500K lines |           0.29 |
| `sort data.txt \| uniq` | 127 MB; 5M lines  |          26.13 |
| `sort -u data.txt`      | 127 MB; 5M lines  |          29.02 |
| `tsv-uniq data.txt`     | 127 MB; 5M lines  |           3.14 |

For more info, see the [tsv-uniq reference](docs/ToolReference.md#tsv-uniq-reference).

## Reading data in R

It's common to perform transformations on data prior to loading into stats and data mining application like R or Pandas. This especially useful when data sets are large, and loading entirely into memory is undesirable. One way to do this is create modified files. In, it can also be done as part of the different read routines, most of which allow reading from a shell command. This enables filtering rows, selecting, sampling, etc. This will work with any command line line tool. Some examples below. These use `read.table` from the base R package, `read_tsv` from the `tidyverse/readr` package, and `fread` from the `data.table` package:
```
> df1 = read.table(pipe("tsv-select -f 1,2,7 data.tsv | tsv-sample -H -n 50000"), sep="\t", header=TRUE, quote="")
> df2 = read_tsv(pipe("tsv-select -f 1,2,7 data.tsv | tsv-sample -H -n 50000"))
> df3 = fread("tsv-select -f 1,2,7 train.tsv | tsv-sample -H -n 50000")
```

The first two use the `pipe` function to create the shell command. `fread` does this automatically.

Note: One common issue is having the PATH environment setup correctly. Depending on setup, the R application might not have the full path normally available in a command shell. See the R documentation for details.

## Using grep as a pre-filter

`tsv-filter` is fast, but a good Unix `grep` implementation is faster. There are good reasons for this, notably, `grep` can ignore line boundaries during initial matching (["why GNU grep is fast", Mike Haertel](https://lists.freebsd.org/pipermail/freebsd-current/2010-August/019310.html)).

Much of the time this won't matter, as `tsv-filter` can process gigabyte files in a couple seconds. However, when working with much larger files or slow I/O devices, the wait may be longer. In these cases, it may be possible to speed things up using `grep`. This will work if there is a string, preferably several characters long, that is found on every line expected in the output, but winnows out a fair number of non-matching lines.

An example, using a subset of files from the [Google Books Ngram Viewer data-sets](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html). In these files, each line holds stats for an ngram, field 2 is the year the stats apply to. In this test, ngram_*.tsv consisted of 39 files, 1.4 billion lines, 27.5 GB of data. To get the lines for the year 1980, this command would be run:
```
$ tsv-filter --str-eq 2:1850 ngram_*.tsv
```

This took 157 seconds on Macbook Pro and output 2770512 records. Grep can also be used:
```
$ grep 1850 ngram_*.tsv
```

This took 37 seconds, but it also produced too many records (2943588), as "1850" appears in places other than the year. But the correct result can generated by using `grep` and `tsv-filter` together:
```
$ grep 1850 ngram_*.tsv | tsv-filter --str-eq 2:1850
```

This also took 37 seconds. `grep` and `tsv-filter` are running in parallel, and `tsv-filter` keeps up easily as it is processing fewer records.

The above example can be done entirely in `grep` by using regular expressions, but it's easy to get wrong and actually slower due to the regular expression use. For example (syntax may be different in your environment):
```
$ grep $'^[^\t]*\t1850\t' ngram_*.tsv
```

This produced the correct results, but took 48 seconds. It is feasible because only string comparisons are needed. It wouldn't work if numeric comparisons were also involved.

Using `grep` as a pre-filter won't always be helpful, that will depend on the specific case, but on occasion it can be quite handy.
